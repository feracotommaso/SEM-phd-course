---
title: "SEM with ordinal variables"
subtitle: "Structural Equation Modeling"
author: "Tommaso Feraco"
date: "Doctoral School in Psychological Sciences • University of Padova — 2023"
bibliography: ../refs/references.bib
csl: ../refs/apa7.csl

format:
  revealjs:
    slide-number: c/t
    self-contained: true
    code-fold: false
    code-overflow: wrap
    theme: [default, ../assets/css/theme.scss, ../assets/css/slides.scss]
    include-in-header: ../assets/slidesheader.html

execute:
  echo: true
  warning: false
  message: false
---

```{r, label="setup", include=FALSE}
library(lavaan)
library(ggplot2)
# used later in the slides
# (keep as in original; install if missing)
suppressPackageStartupMessages({
  library(ggpubr)
  library(reshape2)
  library(corrplot)
  library(MASS)
  library(kableExtra)
})
```

## Credits

Credits to Prof. Massimiliano Pastore for the original slides.


## Outline

- Introduction
- In lavaan
- Model fit
- MG-CFA with ordinal data


## Introduction

In psychology we rarely have data that are normally distributed or that follow a continuous distribution. Our data are probably ordinal or the consequence of ordinal/dichotomous processes:

```{r, warning=FALSE}
set.seed(42); n=10000; items = 30;
score <- rbinom(n,items, rnorm(n,.80,.10))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2, fig.width=9}
latent <- rnorm(n)
d <- data.frame(score, latent)
library(ggplot2)
p1 <- ggplot(data = d, aes(x = score)) +
  geom_histogram(color = "blue", fill = "blue", bins = 100)+theme_bw()+
  xlab("observed scores") +
  ylab("") + theme(axis.ticks.y=element_blank(), 
      axis.text.y=element_blank())
p1
```

`COMMENTS?`


## Introduction

However, we postulate that they are generated from continuous normal latent distributions 
```{r, fig.height=4, fig.width=9, echo=FALSE}
p2 <- ggplot(data = d, aes(latent)) +
  geom_histogram(color = "blue", bins = 100) +theme_bw()+
  xlab("true latent scores") +
  ylab("") + theme(axis.ticks.y=element_blank(), 
      axis.text.y=element_blank())
ggpubr::ggarrange(p2,p1, ncol = 1)
```
`COMMENTS?`


## Likert scales

This also applies to Liker scales, where the difference between reporting a score of 1 or 2, and the difference between reporting a score of 2 or 3 is not the same!
```{r, echo=FALSE, message=FALSE, warning=FALSE, size="tiny"}
set.seed(12)
# Assuming Likert item scores ranging from 1 to 5
score <- rnorm(10000)
likert_data<-data.frame(score,
                        score1 = ifelse(score < -1, 1,
                                       ifelse(score < -.5, 2,
                                              ifelse(score < .5, 3,
                                                     ifelse(score < 2, 4, 5)))),
                        score2 = ifelse(score < -.2, 1,
                                       ifelse(score < .5, 2,
                                              ifelse(score < 1, 3,
                                                     ifelse(score < 1.9, 4, 5)))))
library(ggplot2)
library(likert)
library(dplyr)
d <- data.frame(x1 = as.factor(likert_data$score1),
                x2 = as.factor(likert_data$score2))
asd <- likert(d)
pb <- plot(asd, tipe = "bars", legend.position = "none", xlab = "")
p0 <- ggplot(data = likert_data, aes(x = score)) +
  geom_density()
build <- ggplot_build(p0)
df_breaks <- build$data[[1]] %>% 
  mutate(status = ifelse(x < -1, 1,
                         ifelse(x < -.5, 2,
                                ifelse(x < .5, 3,
                                       ifelse(x < 2, 4, 5)))))
pd1 <- df_breaks %>% 
  ggplot() +
  geom_area(
    aes(x = x,
        y = y,
        fill = as.factor(status))
  ) +
  labs(fill = "response")+ 
  theme_bw() + scale_fill_brewer(palette="Set3")
df_breaks <- build$data[[1]] %>% 
  mutate(status = ifelse(x < -.2, 1,
                         ifelse(x < .5, 2,
                                ifelse(x < 1, 3,
                                       ifelse(x < 1.9, 4, 5)))))
pd2 <- df_breaks %>% 
  ggplot() +
  geom_area(
    aes(x = x,
        y = y,
        fill = as.factor(status))
  ) +
  labs(fill = "response") + 
  theme_bw() + scale_fill_brewer(palette="Set3")
pd <- ggpubr::ggarrange(pd1, pd2, common.legend = T, legend = "bottom")

ph1 <- ggplot(data = likert_data, aes(x = score1)) +
  geom_histogram(binwidth = .5, fill = "orange", color = "black") +scale_color_brewer(palette="Dark2")  +
  theme_bw() + theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )
ph2 <- ggplot(data = likert_data, aes(x = score2)) +
  geom_histogram(binwidth = .5, fill = "orange", color = "black") +scale_color_brewer(palette="Dark2")  +
  theme_bw() + theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )
ph <- ggpubr::ggarrange(ph1, ph2, ncol = 1)

pp <- ggpubr::ggarrange(pb, ph, ncol = 2)
ggpubr::ggarrange(pp, pd, ncol = 1)
```


## The `adaptability` data

And in real data they are often not normally distributed
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(reshape2)
D.ad <- read.csv("../data/Exercise3_2.csv")[,-1]
dad <- melt(D.ad)
ggplot(dad, aes(x = value, fill = "orange", color = "black")) +
  geom_histogram(binwidth = .5) +
  facet_wrap(~ variable) +scale_color_brewer(palette="Dark2")  +
  theme_bw()+ 
  theme(legend.position = "none")
```


## Wrong correlations

Unfortunately, when we use data that follow discrete distributions and treat them as they were continuous, we can fail to recreate true correlation matrices accurately. That's why you usually use polychoric correlations when you calculate correlations with dichotomous or ordinal variables.

```{r, echo=FALSE}
cm <- matrix(c(  1, .50, .50, .50,            
               .50,   1,  .50, .50,      
               .50, .50,    1, .50,   
               .50, .50,  .50,   1), ncol = 4)
set.seed(12)
d <- as.data.frame(MASS::mvrnorm(n=10000, mu = c(0,0,0,0), Sigma = cm))
d1 <- data.frame(x1 = ifelse(d$V1 < -1, 1, ifelse(d$V1 < .5, 2, 3)),
                 x2 = ifelse(d$V2 < -1, 1, ifelse(d$V2 < .5, 2, 3)),
                 x3 = ifelse(d$V3 < -1, 1, ifelse(d$V3 < .5, 2, 3)),
                 x4 = ifelse(d$V4 < -1, 1, ifelse(d$V4 < .5, 2, 3)))
cor1 <- cor(d)
cor2 <- cor(d1)
```

::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5}
corrplot::corrplot(cor1, addCoef.col = 'black', method = 'ellipse', tl.pos = 'd')
```
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5}
corrplot::corrplot(cor2, addCoef.col = 'black', method = 'ellipse', tl.pos = 'd')
```
:::
:::

Data were generated from a multivariate normal distribution using `MASS::mvrnorm` and then manually truncated on a 3-point scale

This, of course, has consequences on SEM parameters, which are based on covariance


## In `lavaan` and SEM

To estimate a model treating items/observations as ordinal data, we need to change the estimationd method
- `ML` is not always accurate with ordinal data (especially with few categories)
- `lavaan`, when `ordered` is `TRUE`, automatically use `DWLS` (diagonally weighted least squares)
- A great alternative is `ULS`, which usually performs better, but has more convergence problems
- We can also use robust ML alternatives, like `MLR`
- Other available estimators: lavaan tutorial on estimators (see course site)


## How to fit a CFA with ordinal data

```{r, message=FALSE}
library(lavaan)
```
```{r, eval=TRUE, tidy=TRUE}
# THE MODEL IS SPECIFIED AS USUAL
mOrd <- "
cb =~ Adaptability_1 + Adaptability_2 + Adaptability_3 + 
      Adaptability_4 + Adaptability_5 + Adaptability_6
em =~ Adaptability_7 + Adaptability_8 + Adaptability_9
em ~~ cb
"
# WE JUST NEED TO ADD
fitOrd <- sem(mOrd, D.ad, std.lv=T,
              estimator = "ULS", # optional
              ordered = colnames(D.ad)) # the list of ord vars
#             ordered = T) # or just "TRUE" if all ordered       
```


## Results (1)

```{r, eval=FALSE, echo=FALSE}
summary(fitOrd, std=T)
```
```{r, size="tiny"}
temp = capture.output(summary(fitOrd, std=T))
cat(c(temp[1:21], "[...]"), sep = "\n")
```


## Results (2)

```{r, size="tiny", echo=FALSE}
temp = capture.output(summary(fitOrd, std=T))
cat(c("[...]", temp[22:40], "[...]"), sep = "\n")
```


## Results (3)

```{r, size="tiny", echo=FALSE}
temp = capture.output(summary(fitOrd, std=T))
cat(c("[...]", temp[41:61], "[...]"), sep = "\n")
```


## Results (4)

```{r, size="tiny", echo=FALSE}
temp = capture.output(summary(fitOrd, std=T))
cat(c("[...]", temp[62:79], "[...]", "[...]"), sep = "\n")
```


## Thresholds (1)

- We can assume that a discrete variable $x$ (expressed with $k$ ordered categories) represents an approximation of a continuous latent variable $\xi$, normally distributed with mean 0.
- Therefore, when we observe $x = i$, it means that the true corresponding value $\xi$ is ranging between two values, i.e.

$$
\alpha_{i-1} < \xi \leq \alpha_i
$$
  where $\alpha_0 = - \infty, \alpha_1 < \alpha_2 < \dots < \alpha_{k-1}$ e $\alpha_k = +\infty$ are the thresholds
- Consequenlty we will have that, given a discrete ordered variable with $k$ possible values, there are $k - 1$ unknown thresholds.


## Thresholds (2)

Thresholds represent the link between the (continuous) latent variable $\xi$ and the observed values (on a discrete scale).   

For example, the item `Adaptability_1` 
```{r, echo=FALSE}
round(parameterTable(fitOrd)[11:16,"est"],2)
```

That we can manually compute as:
```{r, tidy=TRUE}
round(qnorm(cumsum(table(D.ad$Adaptability_1)) /
             sum(table(D.ad$Adaptability_1))),2)
```


## Results (5)

```{r, size="tiny", echo=FALSE}
temp = capture.output(summary(fitOrd, std=T))
cat(c("[...]", temp[99:112], "[...]"), sep = "\n")
```


## Model fit

This works as usual
```{r, tidy=TRUE, eval=FALSE}
fi <- c("npar", "df", "chisq", 
        "cfi", "tli", "nnfi", "agfi", 
        "srmr", "rmsea", "bic", "aic")
fitmeasures(fitOrd, fit.measures = fi)
# OR inspect(fitOrd, "fit")[fi]
```
```{r, echo=FALSE}
fi <- c("npar", "df", "chisq", 
        "cfi", "tli", "nnfi", "agfi", 
        "srmr", "rmsea", "bic", "aic")
fitmeasures(fitOrd, fit.measures = fi)[1:3]
fitmeasures(fitOrd, fit.measures = fi)[4:7]
fitmeasures(fitOrd, fit.measures = fi)[8:11]
```
`BUT YOU CANNOT INTERPRET THEM AS WE USED TO DO!`


## Model fit, references

Some references for model fit with ordinal data:
- RMSEA (doi:10.1080/10705511.2019.1611434) tends to reject models with large datasets and 5-point scales
- CFI and TLI tend to overestimate model fit
- SRMR seems to be less biased 

But there are many contradictory suggestions and it is not easy to navigate them. Look for what you need as simulation studies depend on many variables.   

This (doi:10.1027/2698-1866/a000034) might be a helpful summary/reflection.


## Reviewer 2

![](../assets/images/reviewer2.png){width="94%"}


## Prerequisites

When we test multigroup invariance with ordinal data we assume that the `THRESHOLDS` are also equal between the two groups, but before running the analysis, remember:
- the number of parameters is higher than with continuous data...and you split the data in two or more parts! Be sure you have enough data in each group
- all the observed indicators hold the same categories in each group, otherwise you cannot fit the model


## Steps

The steps that you should follow fo MG-CFA with ordinal data are slightly different:
- **Baseline model**, as the configural model
- **Equal thresholds model**, you should start by forcing the thresholds to be equal across groups
- **Equal loadings and thresholds model**, only now you can fix the loadings to be equal across groups


## In `R`

I use again the adaptability items. I manually added a group variable.
```{r}
D.ad$group <- c(rep("G1", 428), rep("G2", 1083-428))
# 1. FIT THE BASELINE/CONFIGURAL MODEL
fConf <- sem(mOrd, D.ad, std.lv=T, estimator = "ULS",
             ordered = T, group = "group")
# 2. FIT THE FIXED THRESHOLDS MODEL
fTresh<- sem(mOrd, D.ad, std.lv=T, estimator = "ULS",
             ordered = T, group = "group",
             group.equal = c("thresholds"))
# 3. FIT THE FIXED LOADINGS MODEL
fLoad <- sem(mOrd, D.ad, std.lv=T, estimator = "ULS",
             ordered = T, group = "group",
             group.equal = c("thresholds", "loadings"))
```


## Model fit comparison

```{r}
fitTable <- rbind(fitmeasures(fConf, fi),
                  fitmeasures(fTresh, fi),
                  fitmeasures(fLoad, fi))
```
```{r, echo=FALSE, message=FALSE, results='asis'}
library(kableExtra)
rownames(fitTable) <- c("baseline", "thresholds", "loadings")
kbl(round(fitTable, digits = 3)) %>%
   kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed")) %>%
   row_spec(0,bold=TRUE)
```


## Model results

```{r, size="tiny", echo=FALSE, results='asis'}
pt1 <- parameterEstimates(fConf, standardized=T)[1:16, c(1:3, 6)]
pt2 <- parameterEstimates(fConf, standardized=T)[96:111, c(1:3, 6)]
p0 <- rep("||", 16)
pt <- cbind(pt1, p0, pt2)
pt[,c(4,9)] <- round(pt[,c(4,9)],2)
names(pt)[5] <- "||"
kbl(pt) %>%
   kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed")) %>%
   row_spec(0,bold=TRUE) %>%
   add_header_above(c("Group 1" = 4, " " = 1, "Group 2" = 4)) %>%
   pack_rows("Loadings", 1, 9) %>%
  pack_rows("Latent covariance", 10, 10) %>%
  pack_rows("Thresholds", 11, 16) %>%
   row_spec(0,bold=TRUE)
```


## Additional materials

- Svetina et al. (doi:10.1080/10705511.2019.1602776) tutorial, suggestions, and model fit recommendations for MG-CFA with ordinal data 
- Enrico Perinelli held a `psicostat` meeting on the topic following Svetina et al.'s code
