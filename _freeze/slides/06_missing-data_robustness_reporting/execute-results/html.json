{
  "hash": "6aec0d0dce3766addeeebe9dcec1b8dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lesson 6 — When SEM Goes Wrong\"\nsubtitle: \"Robustness, Missing Data, and Reporting in SEM\"\nformat:\n  revealjs:\n    slide-number: c/t\n    code-overflow: wrap\n    theme: [default, ../assets/css/theme.scss, ../assets/css/slides.scss]\ninclude-in-header: ../assets/slidesheader.html\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nbibliography: ../refs/references.bib\ncsl: ../refs/apa7.csl\n---\n\n## Workflow map\n\nSpecify → Identify → **Estimate** → **Evaluate** → Report\n\nToday we practice a *sensitivity mindset*:\n\n> Same model, different reasonable choices → **does the conclusion change?**\n\n---\n\n## Learning objectives\n\nBy the end of today you can:\n\n- Distinguish **MCAR / MAR / MNAR** and what FIML assumes\n- Explain what **MLR** corrects (robust SE + scaled test statistic)\n- Decide when **bootstrap CIs** are needed (and when they’re overkill)\n- Combine **global + local** diagnostics without fishing\n- Write a **defensible reporting paragraph**\n\n---\n\n## Plan for today\n\n1) Simulate a “realistic messy” dataset (skewness + MAR missingness)  \n2) Fit the *same* SEM under different choices  \n3) Compare conclusions (parameters, SE, CIs, fit, local misfit)  \n4) Turn results into reporting decisions\n\n> <span style=\"color:red\">Add figure: one pipeline diagram showing “Same model → different estimation choices → compare conclusions”.</span>\n\n---\n\n# Part I — Build a dataset that can fool you\n\n## Why simulate?\n\nReal SEM work is rarely “clean”:\n\n- indicators are **skewed**\n- residuals are **non-normal**\n- missingness is **not random**\n- indirect effects are **asymmetric**\n\nWe simulate this so we can *see* what changes and what doesn’t.\n\n---\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lavaan)\n\nset.seed(1234)\nN <- 600\n```\n:::\n\n\n---\n\n## Population model (measurement + structural)\n\nA simple measurement-first SEM:\n\n- peer pressure → social comparison  \n- social media → social comparison  \n- social comparison → eating disorder symptoms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pop <- \"\n# Measurement\npeer  =~ 0.80*p1 + 0.70*p2 + 0.60*p3 + 0.70*p4\nmedia =~ 0.70*m1 + 0.80*m2 + 0.60*m3 + 0.70*m4\ncomp  =~ 0.70*c1 + 0.70*c2 + 0.60*c3\neat   =~ 0.70*e1 + 0.60*e2\n\n# Structural\ncomp ~ 0.40*peer + 0.50*media\neat  ~ 0.35*comp\n\"\ndat <- simulateData(model_pop, sample.nobs = N)\n```\n:::\n\n\n---\n\n## Make indicators skewed\n\nWe “Likert-ify” *some* indicators by applying a monotone transform (skewness).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskew_vars <- c(\"p1\",\"p2\",\"m1\",\"m2\",\"c1\")\nfor (v in skew_vars) dat[[v]] <- exp(dat[[v]] / 2)\n```\n:::\n\n\n---\n\n## Quick distribution check\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(dat$p1, main = \"Skewed indicator: p1\", xlab = \"p1\")\n```\n\n::: {.cell-output-display}\n![](06_missing-data_robustness_reporting_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n> <span style=\"color:red\">Add conceptual graphic: normal vs skewed distributions with same mean/variance but different tails.</span>\n\n---\n\n## Add heavy tails (non-normal residual behavior)\n\nWe inject a small number of outliers in two indicators (a common real-life pattern).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nix <- sample(seq_len(N), size = round(0.03*N))  # ~3% outliers\ndat$m4[ix] <- dat$m4[ix] + rnorm(length(ix), mean = 0, sd = 4)\ndat$c3[ix] <- dat$c3[ix] + rnorm(length(ix), mean = 0, sd = 4)\n```\n:::\n\n\n---\n\n## A simple non-normality summary (no extra packages)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskew <- function(x) {\n  x <- x[is.finite(x)]\n  m <- mean(x); s <- sd(x)\n  mean((x - m)^3) / s^3\n}\nkurt_excess <- function(x) {\n  x <- x[is.finite(x)]\n  m <- mean(x); s <- sd(x)\n  mean((x - m)^4) / s^4 - 3\n}\n\nround(c(skew = skew(dat$p1), kurt_excess = kurt_excess(dat$p1)), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       skew kurt_excess \n       2.83       14.79 \n```\n\n\n:::\n:::\n\n\n---\n\n## Create MAR missingness (~20%)\n\nMissingness depends on an *observed* variable (MAR), not on the missing value itself.\n\nWe make e1 and m3 more likely to be missing when peer pressure is high.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\n# a proxy observed score for peer (in real life: a sum score, previous wave, etc.)\npeer_obs <- rowMeans(dat[, c(\"p1\",\"p2\",\"p3\",\"p4\")])\n\np_miss <- plogis(scale(peer_obs))            # 0..1\nmiss   <- runif(N) < (p_miss * 0.45)         # tune to ~20%\n\ndat$e1[miss] <- NA\ndat$m3[miss] <- NA\n\nround(colMeans(is.na(dat)), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n p1  p2  p3  p4  m1  m2  m3  m4  c1  c2  c3  e1  e2 \n0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 0.0 0.2 0.0 \n```\n\n\n:::\n:::\n\n\n---\n\n## Missingness: what would you check?\n\n- % missing per variable\n- patterns (is it concentrated in a subset?)\n- association between missingness and observed variables (supports MAR plausibility)\n\n> <span style=\"color:red\">Add small schematic: MCAR vs MAR vs MNAR (arrows from observed/unobserved to missingness indicator R).</span>\n\n---\n\n# Part II — One SEM, multiple reasonable choices\n\n## The analysis model (same model throughout)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_sem <- \"\n# Measurement\npeer  =~ p1 + p2 + p3 + p4\nmedia =~ m1 + m2 + m3 + m4\ncomp  =~ c1 + c2 + c3\neat   =~ e1 + e2\n\n# Structural\ncomp ~ peer + media\neat  ~ comp\n\"\n```\n:::\n\n\n---\n\n## A helper: extract the same key parameters each time\n\nWe track the *same* hypotheses in every fit:\n\n- peer → comp\n- media → comp\n- comp → eat\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkey_paths <- function(fit) {\n  pe <- parameterEstimates(fit)\n  pe <- pe[pe$op == \"~\" & pe$lhs %in% c(\"comp\",\"eat\"), ]\n  pe[pe$rhs %in% c(\"peer\",\"media\",\"comp\"),\n     c(\"lhs\",\"op\",\"rhs\",\"est\",\"se\",\"z\",\"pvalue\")]\n}\n```\n:::\n\n\n---\n\n# Part III — Missing data: listwise vs FIML\n\n## Concepts (technical, but actionable)\n\n- **MCAR**: missingness unrelated to observed/unobserved → listwise unbiased (but inefficient)\n- **MAR**: missingness depends on observed variables → **FIML OK** (under correct model)\n- **MNAR**: missingness depends on unobserved/missing values → both listwise & FIML can be biased\n\nKey point:\n\n> FIML is not “imputation”. It’s likelihood-based estimation using all available cases **assuming MAR**.\n\n---\n\n## Fit 1 — ML, default missing handling (listwise)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_list <- sem(model_sem, data = dat)  # default: listwise deletion\nfitMeasures(fit_list, c(\"nobs\",\"chisq\",\"df\",\"cfi\",\"tli\",\"rmsea\",\"srmr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n chisq     df    cfi    tli  rmsea   srmr \n56.219 61.000  1.000  1.009  0.000  0.030 \n```\n\n\n:::\n:::\n\n\n---\n\n## Fit 2 — ML + FIML\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_fiml <- sem(model_sem, data = dat, missing = \"fiml\")\nfitMeasures(fit_fiml, c(\"nobs\",\"chisq\",\"df\",\"cfi\",\"tli\",\"rmsea\",\"srmr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n chisq     df    cfi    tli  rmsea   srmr \n54.717 61.000  1.000  1.010  0.000  0.027 \n```\n\n\n:::\n:::\n\n\n---\n\n## Sensitivity check: did the conclusion change?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbind(\n  listwise = key_paths(fit_list),\n  fiml     = key_paths(fit_fiml)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             lhs op   rhs   est    se     z pvalue\nlistwise.14 comp  ~  peer 0.362 0.103 3.523  0.000\nlistwise.15 comp  ~ media 0.424 0.093 4.578  0.000\nlistwise.16  eat  ~  comp 0.430 0.125 3.436  0.001\nfiml.14     comp  ~  peer 0.409 0.089 4.592  0.000\nfiml.15     comp  ~ media 0.450 0.085 5.280  0.000\nfiml.16      eat  ~  comp 0.427 0.124 3.442  0.001\n```\n\n\n:::\n:::\n\n\n::: callout-note\n### Interpretation rule of thumb (today)\nIf your substantive conclusion changes under a reasonable alternative (e.g., listwise → FIML), treat the result as **fragile** and investigate *why*.\n:::\n\n---\n\n# Part IV — Robust estimation: ML vs MLR\n\n## What MLR does (the technical version)\n\nMLR (robust ML) typically:\n\n- keeps **similar point estimates**\n- adjusts **standard errors** using a “sandwich” (empirical) correction\n- reports a **scaled** test statistic (robust χ²) and robust fit indices\n\nUse case:\n\n- non-normality (skewness, heavy tails)\n- mild misspecification\n- “psychology-shaped” data\n\n---\n\n## Fit 3 — MLR + FIML\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_mlr <- sem(model_sem, data = dat,\n               missing = \"fiml\",\n               estimator = \"MLR\")\nfitMeasures(fit_mlr, c(\"nobs\",\"chisq\",\"df\",\"cfi\",\"tli\",\"rmsea\",\"srmr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n chisq     df    cfi    tli  rmsea   srmr \n54.717 61.000  1.000  1.010  0.000  0.027 \n```\n\n\n:::\n:::\n\n\n---\n\n## Sensitivity check: do SE / p-values change?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbind(\n  fiml_ML  = key_paths(fit_fiml),\n  fiml_MLR = key_paths(fit_mlr)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             lhs op   rhs   est    se     z pvalue\nfiml_ML.14  comp  ~  peer 0.409 0.089 4.592  0.000\nfiml_ML.15  comp  ~ media 0.450 0.085 5.280  0.000\nfiml_ML.16   eat  ~  comp 0.427 0.124 3.442  0.001\nfiml_MLR.14 comp  ~  peer 0.409 0.091 4.496  0.000\nfiml_MLR.15 comp  ~ media 0.450 0.146 3.085  0.002\nfiml_MLR.16  eat  ~  comp 0.427 0.125 3.430  0.001\n```\n\n\n:::\n:::\n\n\n::: callout-warning\n### Pitfall\nDon’t mix-and-match reporting:\n- If you estimate **MLR**, report **robust** fit statistics (scaled χ², robust RMSEA/CFI/TLI).\n- Don’t copy/paste the ML χ² from another run.\n:::\n\n---\n\n# Part V — Bootstrap CIs (only when it matters)\n\n## Why bootstrap is special for indirect effects\n\nIndirect effects are products of coefficients:\n\n$ab = a \\times b$\n\nEven if $(a)$ and $(b)$ are roughly normal, $(ab)$ is often **skewed** → normal-theory CIs can be misleading.\n\n---\n\n## (Optional) Add an indirect effect to the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_sem_ind <- paste0(model_sem, \"\\n\\n# Indirect effect\\nind_peer := (comp~peer)*(eat~comp)\\n\")\n```\n:::\n\n\n> <span style=\"color:red\">If lavaan complains about label reuse depending on version, label paths explicitly (a* and b*) and redefine ind := a*b.</span>\n\n---\n\n## Fit 4 — Bootstrap SE/CI (keep small for live teaching)\n\nFor live demos keep bootstrap modest (e.g., 300–800).  \nFor papers, use ~2000+.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_boot <- sem(model_sem, data = dat,\n                missing = \"fiml\",\n                se = \"bootstrap\",\n                bootstrap = 500)\n```\n:::\n\n\n---\n\n## Compare CI for key paths (normal vs bootstrap)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npe_norm <- parameterEstimates(fit_fiml, ci = TRUE)\npe_boot <- parameterEstimates(fit_boot, ci = TRUE, boot.ci.type = \"perc\")\n\nsel <- function(pe) {\n  pe[pe$op == \"~\" & pe$lhs %in% c(\"comp\",\"eat\") &\n       pe$rhs %in% c(\"peer\",\"media\",\"comp\"),\n     c(\"lhs\",\"op\",\"rhs\",\"est\",\"ci.lower\",\"ci.upper\")]\n}\n\nlist(\n  normal_CI = sel(pe_norm),\n  boot_CI   = sel(pe_boot)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$normal_CI\n    lhs op   rhs   est ci.lower ci.upper\n14 comp  ~  peer 0.409    0.235    0.584\n15 comp  ~ media 0.450    0.283    0.616\n16  eat  ~  comp 0.427    0.184    0.670\n\n$boot_CI\n    lhs op   rhs   est ci.lower ci.upper\n14 comp  ~  peer 0.409    0.236    0.601\n15 comp  ~ media 0.450    0.242    0.801\n16  eat  ~  comp 0.427    0.188    0.692\n```\n\n\n:::\n:::\n\n\n::: callout-note\n### Decision heuristic (today)\nBootstrap is most valuable when:\n- your target parameter is a **product** (indirect effects),\n- distributions are skewed / small N,\n- normal CIs would be suspect.\n:::\n\n---\n\n# Part VI — Fit indices: the dangerous comfort\n\n## Global fit (quick recap)\n\nGlobal fit indices summarize average discrepancy:\n\n- χ² (sample-size sensitive)\n- CFI/TLI (incremental)\n- RMSEA (+ CI; small df behavior)\n- SRMR (residual-based)\n\nBut:\n\n> Good global fit does not guarantee good measurement or correct structure.\n\n---\n\n## Local fit: residuals and MI\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual correlations (a small block)\nresid(fit_mlr, type = \"cor\")$cov[1:6, 1:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              p1            p2           p3            p4            m1\np1 -2.220446e-16  2.260205e-02 -0.027764125 -1.156267e-02 -5.688705e-02\np2  2.260205e-02  2.220446e-16  0.008962091 -1.205551e-02  3.437288e-03\np3 -2.776412e-02  8.962091e-03  0.000000000  2.145905e-02 -1.029635e-02\np4 -1.156267e-02 -1.205551e-02  0.021459052 -1.110223e-16  3.037465e-02\nm1 -5.688705e-02  3.437288e-03 -0.010296345  3.037465e-02  2.220446e-16\nm2  1.572566e-02  3.943457e-02  0.010333999 -2.492806e-02  1.005055e-02\n            m2\np1  0.01572566\np2  0.03943457\np3  0.01033400\np4 -0.02492806\nm1  0.01005055\nm2  0.00000000\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodificationIndices(fit_mlr, sort. = TRUE)[1:10, c(\"lhs\",\"op\",\"rhs\",\"mi\",\"epc\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     lhs op rhs    mi    epc\n132   p4 ~~  e2 4.658  0.116\n147   m2 ~~  e2 4.044 -0.074\n94    p1 ~~  m1 3.794 -0.047\n84   eat =~  m1 3.541  0.124\n109   p2 ~~  c1 3.434 -0.042\n125   p4 ~~  m2 3.237 -0.064\n85   eat =~  m2 3.024 -0.113\n56  peer =~  c1 2.961 -0.207\n96    p1 ~~  m3 2.841 -0.061\n101   p1 ~~  e1 2.573  0.065\n```\n\n\n:::\n:::\n\n\n> <span style=\"color:red\">Add schematic: “CFI looks fine” but highlight a single large residual correlation and its substantive interpretation.</span>\n\n---\n\n## A disciplined respecification rule\n\nOnly consider modifications that are:\n\n1) theoretically defensible  \n2) consistent with measurement-first logic  \n3) reported transparently (what was changed and why)\n\n::: callout-warning\n### Pitfall: “MI shopping”\nIf you add correlated errors because they “fix RMSEA”, you can end up fitting noise.\n:::\n\n---\n\n# Part VII — Reporting like a researcher\n\n## What you must report (minimum)\n\n- Model specification (measurement + structural)\n- Estimator (ML / MLR / DWLS / ULS / …)\n- Missing data handling (listwise / FIML / …) + assumption (MAR)\n- χ²(df), p (robust/scaled if applicable)\n- CFI, TLI, RMSEA (+ CI), SRMR\n- Any respecifications (with theory rationale)\n- If bootstrap: type + number of draws + CI type\n\n---\n\n## Example reporting paragraph (template)\n\n> The SEM was estimated in `lavaan` using robust maximum likelihood (MLR) with FIML for missing data under a MAR assumption. Model fit was evaluated using the scaled χ² test and robust fit indices (CFI, TLI, RMSEA with 90% CI, SRMR). Key parameters were interpreted based on standardized estimates and robust standard errors. Where relevant, confidence intervals were obtained via bootstrap percentile CIs (B = 500 for teaching; ≥ 2000 for publication).\n\n---\n\n## Take-home: the sensitivity mindset\n\nFor any important claim, ask:\n\n- Does it survive **FIML vs listwise**?\n- Does it survive **ML vs MLR**?\n- Does it survive **bootstrap vs normal CI** (when relevant)?\n- Does it survive **local diagnostics** (residuals/MI)?\n\nIf not, don’t panic—**learn what the data are telling you**.\n\n---\n\n## Exercises → Lab 6\n\nIn the lab you will:\n\n1) Increase missingness to ~40% and re-run the sensitivity checks  \n2) Simulate MNAR and compare to MAR  \n3) Identify 1–2 large MIs, justify (or reject) a modification  \n4) Write a short “Methods + Results” reporting paragraph\n\n> <span style=\"color:red\">Link placeholder: add a direct link to /labs/06_robustness_lab.qmd once created.</span>\n\n---\n\n## 3 things to remember\n\n1) Missing data handling can change conclusions — **check sensitivity**  \n2) Robust SE protect against inflated significance — **don’t trust ML by default**  \n3) Fit indices are diagnostics, not verdicts — **always check local fit**\n\n---\n\n## Further reading (optional)\n\n- Missing data in SEM (FIML, MAR assumptions)\n- Robust estimation (MLR/MLM and non-normality)\n- Bootstrap inference for indirect effects\n- Reporting standards for SEM in psychology\n\n> <span style=\"color:red\">Add 2–3 concrete citations once we confirm keys in refs/references.bib.</span>\n\n---\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}